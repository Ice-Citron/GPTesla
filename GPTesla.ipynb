{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5a4506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Update package list and install Python 3.9\\n!sudo apt-get update\\n!sudo apt-get install -y python3.9\\n\\n# Set up update-alternatives to point to the new Python version\\n!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\\n!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\\n\\n# Select the new Python version as default\\n!sudo update-alternatives --config python3\\n\\n# Verify the Python version\\n!python3 --version\\n\\n# Upgrade pip and install necessary packages\\n!python3 -m pip install --upgrade pip\\n!python3 -m pip install jupyter\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%capture\n",
    "\n",
    "# run manually in terminal in the future\n",
    "\"\"\"\n",
    "# Update package list and install Python 3.9\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3.9\n",
    "\n",
    "# Set up update-alternatives to point to the new Python version\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n",
    "\n",
    "# Select the new Python version as default\n",
    "!sudo update-alternatives --config python3\n",
    "\n",
    "# Verify the Python version\n",
    "!python3 --version\n",
    "\n",
    "# Upgrade pip and install necessary packages\n",
    "!python3 -m pip install --upgrade pip\n",
    "!python3 -m pip install jupyter\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2683e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.5\n",
      "Python 3.9.5\n"
     ]
    }
   ],
   "source": [
    "# Verify the installation\n",
    "!python --version\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e00fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "!git clone https://github.com/nlp-with-transformers/notebooks.git\n",
    "%cd notebooks\n",
    "#from install import *\n",
    "#install_requirements(is_chapter6=True)\n",
    "\"\"\"\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#\"\"\"\n",
    "!pip install transformers==4.41.2\n",
    "!pip install datasets==2.20.0\n",
    "\n",
    "!pip install pyarrow==16.0\n",
    "!pip install requests==2.32.3\n",
    "\n",
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
    "\n",
    "!pip install importlib-metadata\n",
    "\n",
    "!pip install accelerate -U\n",
    "\n",
    "!pip install psutil==6.0.0\n",
    "\n",
    "\n",
    "# Specific to Nvidia Instance\n",
    "!pip install matplotlib==3.9.1\n",
    "!pip install --upgrade cython\n",
    "!pip install --upgrade --force-reinstall ipython\n",
    "!pip install pickleshare\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a106d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers v4.41.2\n",
      "Using datasets v2.20.0\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "setup_chapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6551594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyarrow\n",
      "Version: 16.0.0\n",
      "Summary: Python library for Apache Arrow\n",
      "Home-page: https://arrow.apache.org/\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache License, Version 2.0\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: numpy\n",
      "Required-by: datasets\n",
      "---\n",
      "Name: requests\n",
      "Version: 2.32.3\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: certifi, charset-normalizer, idna, urllib3\n",
      "Required-by: datasets, huggingface-hub, jupyterlab_server, transformers\n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.41.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "---\n",
      "Name: datasets\n",
      "Version: 2.20.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyarrow-hotfix, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: \n",
      "---\n",
      "Name: torch\n",
      "Version: 2.3.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, torchaudio, torchvision\n",
      "---\n",
      "Name: torchaudio\n",
      "Version: 2.3.0\n",
      "Summary: An audio package for PyTorch\n",
      "Home-page: https://github.com/pytorch/audio\n",
      "Author: Soumith Chintala, David Pollack, Sean Naren, Peter Goldsborough, Moto Hira, Caroline Chen, Jeff Hwang, Zhaoheng Ni, Xiaohui Zhang\n",
      "Author-email: soumith@pytorch.org\n",
      "License: \n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: torch\n",
      "Required-by: \n",
      "---\n",
      "Name: importlib_metadata\n",
      "Version: 8.0.0\n",
      "Summary: Read metadata from Python packages\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \"Jason R. Coombs\" <jaraco@jaraco.com>\n",
      "License: \n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: zipp\n",
      "Required-by: jupyter-lsp, jupyter_client, jupyterlab, jupyterlab_server, nbconvert\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "# Verifying packages installed are now up to date\n",
    "!pip show pyarrow requests transformers datasets torch torchaudio importlib-metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef59203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  9 14:04:47 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              53W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              55W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              53W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              57W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fd08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24faf8b9",
   "metadata": {},
   "source": [
    "# Training Transformers from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac87ce2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5b2ebcbbc743f6897e33c68aa7b03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8876b8136cd4f5b8b04e5a563dacb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c22b1bba2094780a876f1182c72bc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b3725cc034486b9432e3a3627ac2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ef2005cd4a416c9ea16a829cbe4b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e08b8c04424412b26f7e2a882b3bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ae5cc848824735b0b76d1a77e37048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee864f22ae504b659d1cedbb907e591c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452d1953b7844fdf95bf81eb6bb0f72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a31ecd891f4c4b9f90a33b3b107bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9498e728204ac4ba32523b51e3ab39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f98e77e8f642f09c65f2df50778b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc5069f2c1849179d8755d5feedc7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954a33fe4430433891590dcbb6cf21c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First comparing gpt and gpt-2\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb003645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT  size: 116.5M parameters\n",
      "GPT2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "# calculates and shows model size\n",
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eeb99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06424b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT completions:\n",
      "1.\n",
      "When they came back, she 'd want to know what he was going to do.\n",
      " \" it 'll teach your brother a lesson, \" he said, and then he looked pointedly\n",
      "at my arm and the blood he 'd poured over my skin. \"\n",
      "2.\n",
      "When they came back, a little boy with a black nose and brown eyes who had\n",
      "started hanging around the neighborhood on special occasions and who lived off\n",
      "of the property. he wore a football jersey with the letters \" the black diamond\n",
      "\" written on it.\n",
      "3.\n",
      "When they came back, and they were still there. \"\n",
      " jake nodded.\n",
      " i shrugged and sat back down. \" so then, they did come back, \" i said,\n",
      "thinking. \" but... \"\n",
      " \" what's her name?\n",
      "\n",
      "GPT-2 completions:\n",
      "1.\n",
      "When they came back the body was still on fire, and there were a lot of people\n",
      "who had died. The smell of diesel had been getting worse. No one was doing well.\n",
      "The streets were deserted, with no water and no running water\n",
      "2.\n",
      "When they came back to the door, they came back with the fire extinguisher.\n",
      "\n",
      "The man, who was carrying only a small gun, shot at the gas and the man's car.\n",
      "As soon as the gas came out of the\n",
      "3.\n",
      "When they came back to the ship, Raine had a choice of waiting or risking an\n",
      "extra ship while he and his crew kept him on the island. Raine got the choice of\n",
      "waiting or having the opportunity to go see the island where he\n"
     ]
    }
   ],
   "source": [
    "# The function below converts the output from the transformers which are in the form of a list to a continuous text for printing.\n",
    "                # function below is to show effects of skewed dataset, since bookcorpus which GPT trained on was very skewed towards romance genre, we can see how different\n",
    "                # their respective output themes are\n",
    "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences,\n",
    "               clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75892e71",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a Custom Code dataset\n",
    "\n",
    "This will be achived using Google Cloud's BigQuery\n",
    "- nevermind. Managed to get google cloud bigquery working. However, seems to be quota issues with free tier account. Not bothered with doing this the hard way so easy way it is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31cd5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8faa413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'codeparrot'...\n",
      "remote: Enumerating objects: 196, done.\u001b[K\n",
      "remote: Total 196 (delta 0), reused 0 (delta 0), pack-reused 196 (from 1)\u001b[K\n",
      "Receiving objects: 100% (196/196), 28.24 KiB | 14.12 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/transformersbook/codeparrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7af01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf639a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50275c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07570f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffba942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9112e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1564c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ac042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
